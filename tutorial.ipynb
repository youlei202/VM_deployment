{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2017 reinforce.io. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import logging\n",
    "import unittest\n",
    "\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "\n",
    "class TestTutorialCode(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Validation of random code snippets as to be notified when old blog posts need to be changed.\n",
    "    \"\"\"\n",
    "\n",
    "    class MyClient(object):\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            pass\n",
    "\n",
    "        def get_state(self):\n",
    "            import numpy as np\n",
    "            return np.random.rand(10)\n",
    "\n",
    "        def execute(self, action):\n",
    "            pass\n",
    "\n",
    "    def test_reinforceio_homepage(self):\n",
    "        \"\"\"\n",
    "        Code example from the homepage and README.md.\n",
    "        \"\"\"\n",
    "\n",
    "        from tensorforce.agents import TRPOAgent\n",
    "\n",
    "        # Create a Trust Region Policy Optimization agent\n",
    "        agent = TRPOAgent(\n",
    "            states_spec=dict(shape=(10,), type='float'),\n",
    "            actions_spec=dict(type='int', num_actions=2),\n",
    "            network_spec=[dict(type='dense', size=50), dict(type='dense', size=50)],\n",
    "            batch_size=100,\n",
    "        )\n",
    "\n",
    "        # Get new data from somewhere, e.g. a client to a web app\n",
    "        client = TestTutorialCode.MyClient('http://127.0.0.1', 8080)\n",
    "\n",
    "        # Poll new state from client\n",
    "        state = client.get_state()\n",
    "\n",
    "        # Get prediction from agent, execute\n",
    "        action = agent.act(states=state)\n",
    "        reward = client.execute(action)\n",
    "\n",
    "        # Add experience, agent automatically updates model according to batch size\n",
    "        agent.observe(reward=reward, terminal=False)\n",
    "        agent.close()\n",
    "\n",
    "    def test_blogpost_introduction(self):\n",
    "        \"\"\"\n",
    "        Test of introduction blog post examples.\n",
    "        \"\"\"\n",
    "        import tensorflow as tf\n",
    "        import numpy as np\n",
    "\n",
    "        ### DQN agent example\n",
    "        from tensorforce.agents import DQNAgent\n",
    "\n",
    "        # Network is an ordered list of layers\n",
    "        network_spec = [dict(type='dense', size=32), dict(type='dense', size=32)]\n",
    "\n",
    "        # Define a state\n",
    "        states = dict(shape=(10,), type='float')\n",
    "\n",
    "        # Define an action\n",
    "        actions = dict(type='int', num_actions=5)\n",
    "\n",
    "        agent = DQNAgent(\n",
    "            states_spec=states,\n",
    "            actions_spec=actions,\n",
    "            network_spec=network_spec,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8,\n",
    "            first_update=100,\n",
    "            target_sync_frequency=10\n",
    "        )\n",
    "\n",
    "        agent.close()\n",
    "\n",
    "        ### Code block: multiple states\n",
    "        states = dict(\n",
    "            image=dict(shape=(64, 64, 3), type='float'),\n",
    "            caption=dict(shape=(20,), type='int')\n",
    "        )\n",
    "\n",
    "        # DQN does not support multiple states. Omit test for now.\n",
    "        # agent = DQNAgent(config=config)\n",
    "\n",
    "        ### Code block: DQN observer function\n",
    "\n",
    "        def observe(self, reward, terminal):\n",
    "            super(DQNAgent, self).observe(reward, terminal)\n",
    "            if self.timestep >= self.first_update \\\n",
    "                    and self.timestep % self.target_update_frequency == 0:\n",
    "                self.model.update_target()\n",
    "\n",
    "        ### Code block: Network config JSON\n",
    "\n",
    "        network_json = \"\"\"\n",
    "        [\n",
    "            {\n",
    "                \"type\": \"conv2d\",\n",
    "                \"size\": 32,\n",
    "                \"window\": 8,\n",
    "                \"stride\": 4\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"conv2d\",\n",
    "                \"size\": 64,\n",
    "                \"window\": 4,\n",
    "                \"stride\": 2\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"flatten\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"dense\",\n",
    "                \"size\": 512\n",
    "            }\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        ### Test json\n",
    "\n",
    "        import json\n",
    "        network_spec = json.loads(network_json)\n",
    "\n",
    "        ### Code block: Modified dense layer\n",
    "\n",
    "        modified_dense = \"\"\"\n",
    "        [\n",
    "            {\n",
    "                \"type\": \"dense\",\n",
    "                \"size\": 64,\n",
    "                \"bias\": false,\n",
    "                \"activation\": \"selu\",\n",
    "                \"l2_regularization\": 0.001\n",
    "            }\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        ### Test json\n",
    "        network_spec = json.loads(modified_dense)\n",
    "\n",
    "        ### Code block: Own layer type\n",
    "        from tensorforce.core.networks import Layer\n",
    "\n",
    "        class BatchNormalization(Layer):\n",
    "\n",
    "            def __init__(self, variance_epsilon=1e-6, scope='batchnorm', summary_labels=None):\n",
    "                super(BatchNormalization, self).__init__(scope=scope, summary_labels=summary_labels)\n",
    "                self.variance_epsilon = variance_epsilon\n",
    "\n",
    "            def tf_apply(self, x, update):\n",
    "                mean, variance = tf.nn.moments(x, axes=tuple(range(x.shape.ndims - 1)))\n",
    "                return tf.nn.batch_normalization(\n",
    "                    x=x,\n",
    "                    mean=mean,\n",
    "                    variance=variance,\n",
    "                    offset=None,\n",
    "                    scale=None,\n",
    "                    variance_epsilon=self.variance_epsilon\n",
    "                )\n",
    "\n",
    "        ### Test own layer\n",
    "\n",
    "        states = dict(shape=(10,), type='float')\n",
    "        network_spec = [\n",
    "            {'type': 'dense', 'size': 32},\n",
    "            {'type': BatchNormalization, 'variance_epsilon': 1e-9}\n",
    "        ]\n",
    "\n",
    "        agent = DQNAgent(\n",
    "            states_spec=states,\n",
    "            actions_spec=actions,\n",
    "            network_spec=network_spec,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8\n",
    "        )\n",
    "\n",
    "        agent.close()\n",
    "\n",
    "        ### Code block: Own network builder\n",
    "        from tensorforce.core.networks import Network\n",
    "\n",
    "        class CustomNetwork(Network):\n",
    "\n",
    "            def tf_apply(self, x, internals, update, return_internals=False):\n",
    "                image = x['image']  # 64x64x3-dim, float\n",
    "                caption = x['caption']  # 20-dim, int\n",
    "                initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01, dtype=tf.float32)\n",
    "\n",
    "                # CNN\n",
    "                weights = tf.get_variable(name='W1', shape=(3, 3, 3, 16), initializer=initializer)\n",
    "                image = tf.nn.conv2d(image, filter=weights, strides=(1, 1, 1, 1), padding='SAME')\n",
    "                image = tf.nn.relu(image)\n",
    "                image = tf.nn.max_pool(image, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')\n",
    "\n",
    "                weights = tf.get_variable(name='W2', shape=(3, 3, 16, 32), initializer=initializer)\n",
    "                image = tf.nn.conv2d(image, filter=weights, strides=(1, 1, 1, 1), padding='SAME')\n",
    "                image = tf.nn.relu(image)\n",
    "                image = tf.nn.max_pool(image, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')\n",
    "\n",
    "                image = tf.reshape(image, shape=(-1, 16 * 16, 32))\n",
    "                image = tf.reduce_mean(image, axis=1)\n",
    "\n",
    "                # LSTM\n",
    "                weights = tf.get_variable(name='W3', shape=(30, 32), initializer=initializer)\n",
    "                caption = tf.nn.embedding_lookup(params=weights, ids=caption)\n",
    "                lstm = tf.contrib.rnn.LSTMCell(num_units=32)\n",
    "                caption, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=caption, dtype=tf.float32)\n",
    "                caption = tf.reduce_mean(caption, axis=1)\n",
    "\n",
    "                # Combination\n",
    "                if return_internals:\n",
    "                    return tf.multiply(image, caption), list()\n",
    "                else:\n",
    "                    return tf.multiply(image, caption)\n",
    "\n",
    "        ### Test own network builder\n",
    "\n",
    "        states = dict(\n",
    "            image=dict(shape=(64, 64, 3), type='float'),\n",
    "            caption=dict(shape=(20,), type='int')\n",
    "        )\n",
    "\n",
    "        agent = DQNAgent(\n",
    "            states_spec=states,\n",
    "            actions_spec=actions,\n",
    "            network_spec=CustomNetwork,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8\n",
    "        )\n",
    "\n",
    "        agent.close()\n",
    "\n",
    "        ### Code block: LSTM function\n",
    "        from tensorforce.core.networks import Layer\n",
    "\n",
    "        class Lstm(Layer):\n",
    "\n",
    "            def __init__(self, size, scope='lstm', summary_labels=()):\n",
    "                self.size = size\n",
    "                super(Lstm, self).__init__(num_internals=1, scope=scope, summary_labels=summary_labels)\n",
    "\n",
    "            def tf_apply(self, x, update, state):\n",
    "                state = tf.contrib.rnn.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])\n",
    "                self.lstm_cell = tf.contrib.rnn.LSTMCell(num_units=self.size)\n",
    "\n",
    "                x, state = self.lstm_cell(inputs=x, state=state)\n",
    "\n",
    "                internal_output = tf.stack(values=(state.c, state.h), axis=1)\n",
    "                return x, (internal_output,)\n",
    "\n",
    "            def internals_input(self):\n",
    "                return super(Lstm, self).internals_input() + [tf.placeholder(dtype=tf.float32, shape=(None, 2, self.size))]\n",
    "\n",
    "            def internals_init(self):\n",
    "                return super(Lstm, self).internals_init() + [np.zeros(shape=(2, self.size))]\n",
    "\n",
    "        ### Test LSTM\n",
    "        states = dict(shape=(10,), type='float')\n",
    "        network_spec = [\n",
    "            {'type': 'flatten'},\n",
    "            {'type': Lstm, 'size': 10}\n",
    "        ]\n",
    "\n",
    "        agent = DQNAgent(\n",
    "            states_spec=states,\n",
    "            actions_spec=actions,\n",
    "            network_spec=network_spec,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8\n",
    "        )\n",
    "\n",
    "        agent.close()\n",
    "\n",
    "        ### Preprocessing configuration\n",
    "        states = dict(shape=(84, 84, 3), type='float')\n",
    "        states_preprocessing_spec = [\n",
    "            dict(\n",
    "                type='image_resize',\n",
    "                width=84,\n",
    "                height=84\n",
    "            ),\n",
    "            dict(\n",
    "                type='grayscale'\n",
    "            ),\n",
    "            dict(\n",
    "                type='normalize'\n",
    "            )\n",
    "            # sequence preprocessing is temporarily broken\n",
    "            # ,\n",
    "            # dict(\n",
    "            #    type='sequence',\n",
    "            #    length=4\n",
    "            # )\n",
    "        ]\n",
    "\n",
    "        ### Test preprocessing configuration\n",
    "\n",
    "        agent = DQNAgent(\n",
    "            states_spec=states,\n",
    "            actions_spec=actions,\n",
    "            network_spec=network_spec,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8,\n",
    "            first_update=100,\n",
    "            target_sync_frequency=50,\n",
    "            states_preprocessing_spec=states_preprocessing_spec\n",
    "        )\n",
    "\n",
    "        agent.close()\n",
    "\n",
    "        ### Code block: Continuous action exploration\n",
    "\n",
    "        exploration = dict(\n",
    "            type='ornstein_uhlenbeck',\n",
    "            sigma=0.1,\n",
    "            mu=0,\n",
    "            theta=0.1\n",
    "        )\n",
    "\n",
    "         ### Test continuous action exploration\n",
    "        agent = DQNAgent(\n",
    "            states_spec=states,\n",
    "            actions_spec=actions,\n",
    "            network_spec=network_spec,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8,\n",
    "            explorations_spec=exploration\n",
    "        )\n",
    "\n",
    "        agent.close()\n",
    "\n",
    "        ### Code block: Discrete action exploration\n",
    "\n",
    "        exploration = dict(\n",
    "            type='epsilon_decay',\n",
    "            initial_epsilon=1.0,\n",
    "            final_epsilon=0.01,\n",
    "            timesteps=1e6\n",
    "        )\n",
    "\n",
    "        ### Test discrete action exploration\n",
    "        agent = DQNAgent(\n",
    "            states_spec=states,\n",
    "            actions_spec=actions,\n",
    "            network_spec=network_spec,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8,\n",
    "            explorations_spec=exploration\n",
    "        )\n",
    "\n",
    "        agent.close()\n",
    "\n",
    "    def test_blogpost_introduction_runner(self):\n",
    "        from tensorforce.environments.minimal_test import MinimalTest\n",
    "        from tensorforce.agents import DQNAgent\n",
    "        from tensorforce.execution import Runner\n",
    "\n",
    "        environment = MinimalTest(specification={'int': ()})\n",
    "\n",
    "        network_spec = [\n",
    "            dict(type='dense', size=32)\n",
    "        ]\n",
    "\n",
    "        agent = DQNAgent(\n",
    "            states_spec=environment.states,\n",
    "            actions_spec=environment.actions,\n",
    "            network_spec=network_spec,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8,\n",
    "            first_update=100,\n",
    "            target_sync_frequency=50\n",
    "        )\n",
    "        runner = Runner(agent=agent, environment=environment)\n",
    "\n",
    "        def episode_finished(runner):\n",
    "            if runner.episode % 100 == 0:\n",
    "                print(sum(runner.episode_rewards[-100:]) / 100)\n",
    "            return runner.episode < 100 \\\n",
    "                or not all(reward >= 1.0 for reward in runner.episode_rewards[-100:])\n",
    "\n",
    "        # runner.run(episodes=1000, episode_finished=episode_finished)\n",
    "        runner.run(episodes=10, episode_finished=episode_finished)  # Only 10 episodes for this test\n",
    "\n",
    "        ### Code block: next\n",
    "        agent = DQNAgent(\n",
    "            states_spec=environment.states,\n",
    "            actions_spec=environment.actions,\n",
    "            network_spec=network_spec,\n",
    "            memory=dict(\n",
    "                type='replay',\n",
    "                capacity=1000\n",
    "            ),\n",
    "            batch_size=8,\n",
    "            first_update=100,\n",
    "            target_sync_frequency=50\n",
    "        )\n",
    "\n",
    "        # max_episodes = 1000\n",
    "        max_episodes = 10  # Only 10 episodes for this test\n",
    "        max_timesteps = 2000\n",
    "\n",
    "        episode = 0\n",
    "        episode_rewards = list()\n",
    "\n",
    "        while True:\n",
    "            state = environment.reset()\n",
    "            agent.reset()\n",
    "\n",
    "            timestep = 0\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                action = agent.act(states=state)\n",
    "                state, terminal, reward = environment.execute(actions=action)\n",
    "                agent.observe(terminal=terminal, reward=reward)\n",
    "\n",
    "                timestep += 1\n",
    "                episode_reward += reward\n",
    "\n",
    "                if terminal or timestep == max_timesteps:\n",
    "                    break\n",
    "\n",
    "            episode += 1\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "            if all(reward >= 1.0 for reward in episode_rewards[-100:]) or episode == max_episodes:\n",
    "                break\n",
    "\n",
    "        agent.close()\n",
    "        environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
